<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/x-icon" href="assets/favicon.png">
    <title>Learning Humanoid Locomotion with Transformers</title>
</head>
<body>
<div id="title_slide">
    <div class="title_left">
        <h1>Learning Humanoid Locomotion<br> with Transformers</h1>
        <div class="author-container">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic<sup>*</sup></a></div>
            <div class="grid-item"><a href="https://tetexiao.com/">Tete Xiao<sup>*</sup></a></div>
            <div class="grid-item"><a href="https://bikezhang106.github.io/">Bike Zhang<sup>*</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>†</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>†</sup></a></div>
            <div class="grid-item"><a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath<sup>†</sup></a></div>
        </div>
        <div class="mobile-author-container-1">
            <div class="grid-item"><a href="https://people.eecs.berkeley.edu/~ilija/">Ilija Radosavovic*</a></div>
            <div class="grid-item"><a href="https://tetexiao.com/">Tete Xiao*</a></div>
            <div class="grid-item"><a href="https://bikezhang106.github.io/">Bike Zhang*</a></div>
        </div>
        <div class="mobile-author-container-2">
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell<sup>†</sup></a></div>
            <div class="grid-item"><a href="http://people.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>†</sup></a></div>
            <div class="grid-item"><a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath<sup>†</sup></a></div>
        </div>
        <div class="contribution">
            Authors contributed equally and listed in alphabetical order</p>
        </div>
        <div class="berkeley">
            <p>University of California, Berkeley</p>
        </div>
        <div class="button-container">
            <a href="http://arxiv.org/abs/2303.03381" class="button">Paper</a>
            <a href="https://youtu.be/nzDkRjSPU_0" class="button">Video</a>
        </div>

        <br>
        <div class="teaser">
            <div class="video_container">
                <video autoplay muted playsinline loop controls preload="metadata">
                    <source src="assets/digit/yoga_ball.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div id="abstract" class="grid-container">
            <p>
                We present a sim-to-real learning-based approach for real-world humanoid locomotion. Our controller is a causal Transformer trained by autoregressive prediction of future actions from the history of observations and actions. We hypothesize that the observation-action history contains useful information about the world that a powerful Transformer model can use to adapt its behavior in-context, without updating its weights. We do not use state estimation, dynamics models, trajectory optimization, reference trajectories, or pre-computed gait libraries. Our controller is trained with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deployed to the real world zero-shot. We evaluate our approach in high-fidelity simulation and successfully deploy it to the real robot as well. To the best of our knowledge, this is the first demonstration of a fully learning-based method for real-world full-sized humanoid locomotion.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">

    <h1>Transformers for Humanoid Locomotion</h1>

    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/method.mp4" type="video/mp4">
            </video>
            <div class="caption" style="margin-top: -2.0vw">
                <p> Our controller is a causal Transformer trained by autoregressive prediction of future actions from the observation-action history.</p>
            </div>
        </div>
    </div>

    <p>
        We explore a learning-based approach for humanoid locomotion. We present a Transformer-based controller that predicts future actions autoregressively from the history of past observations and actions.  
        We hypothesize that the history of observations and actions implicitly encodes the information about the world that a powerful Transformer model can use to adapt its behavior dynamically at test time. For example, the model can use the history of desired vs actual states to figure out how to adjust its actions to better achieve future states. This can be seen as a form of in-context learning (changing model behavior without updating the model parameters).
    </p>

    <h1>Massively Parallel Training in Simulation</h1>
    <p>
        Our model is trained with large-scale model-free reinforcement learning (RL) on an ensemble of randomized environments in simulation.
        We leverage fast GPU simulation powered by IsaacGym and parallelize training across multiple GPUs and thousands of environments.
        Thanks to this, we are able to collect a large number of samples for training (order of 10 Billion in about a day).
    </p>
    <div class="allegrofail">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/sim.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p> We train our policies on various terrain types, including planes, rough planes, and smooth slopes. Our robots execute a variety of randomly sampled walking commands such as walking forward, sideward, turning, or a combination thereof. </p>
            </div>
        </div>
    </div>

    <h1>Zero-Shot Transfer to the the Real World</h1>

    <p>
        We evaluate our learnt policies on a real humanoid robot. 
        We find that our policies trained entirely in simulation are able to transfer to the real world zero-shot.
    </p>

    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/forward.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking forward and backward</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/backward.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Walks on Different Surfaces</h1>

    <p>
    A humanoid robot should be able to walk over different terrains. 
    To assess the capabilities of our transformer-based controller in this regard, we conducted a series of experiments on different terrains in the laboratory.
    In each experiment, we command the robot to walk forward and vary the terrain type.
    We first consider flat terrains with variation in friction (slippery plastic floor and carpet) and roughness (wrapping bags and cables spread over the floor).
    We find that our controller is able to handle these types of terrains, with the slippery plastic floor being the most challenging.
    </p>
    <br>

    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/digit/floor.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/digit/carpet.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/foam.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking over flat terrains with different friction (top) and roughness (bottom)</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/cable.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Walks over Uneven Terrains</h1>
    <p>
        Next, we consider walking over slopes and small steps.
        We note that our robot was not trained on steps in simulation.
        We observe that our controller initially makes a mistake when ascending the step, but quickly corrects and raises the leg higher and faster on the second attempt.
    </p>

    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/slope.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Walking over slopes (left) and steps (right)</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/stair.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Carries Payloads</h1>
    <p>
      We also evaluate the walking behavior when carrying loads of varying mass and shape.
      Notice that our walking behaviors exhibit an emergent arm swing for balancing.
      Placing loads on an arm interferes with this and requies the robot to adapt accordingly.
    </p>

    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/digit/empty_backpack.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/digit/trash_bag.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/yorker_bag.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Carrying loads of varying mass and shape</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/TJ_bag.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Is Robust to Disturbances</h1>

    <p>
        Finally, we test the robustness of policies to sudden external forces. 
        We push the robot with a wooden stick or throw light cardboard boxes at it.
    </p>
    <div class="teleop">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/stick.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Applying suddent external forces</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/digit/mujoco_box.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1> Failure Cases </h1>
    <p> 
        While our policies show promising singal in the real world, they are certainly not without limitations.
        For example, we perform an experiment to test the limits of the robustness of our controller.
        We tie a cable to the back of the robot, command the robot to walk forward, and pull the robot backward.
        We see that the robot is fairly robust but falls eventually when pulled very hard.
    </p>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/digit/failure.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Pulling a robot with a rope until it falls</p>
            </div>
        </div>
    </div>

    <h1>BibTeX</h1>
    <p class="bibtex">
        @article{HumanoidTransformer2023,<br>
        &nbsp;&nbsp;title={Learning Humanoid Locomotion with Transformers},<br>
        &nbsp;&nbsp;author={Ilija Radosavovic and Tete Xiao and Bike Zhang and Trevor Darrell and Jitendra Malik and Koushil Sreenath},<br>
        &nbsp;&nbsp;year={2023},<br>
        &nbsp;&nbsp;journal={arXiv:2303.03381}<br>
        }
    </p>
</div>
<script type="text/javascript">
    /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
    document.querySelector('video').defaultPlaybackRate = 1.0;
    document.querySelector('video').play();

    var videos =document.querySelectorAll('video');
    for (var i=0;i<1;i++)
    {
        videos[i].playbackRate = 1.0;
    }
</script>
<script>
    /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
    var videos = document.getElementsByTagName("video");

    function checkScroll() {
        var fraction = 0.5; // Play when 70% of the player is visible.

        for(var i = 0; i < 1; i++) {  // only apply to the first video

            var video = videos[i];

            var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
                b = y + h, //bottom
                visibleX, visibleY, visible;

            visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
            visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

            visible = visibleX * visibleY / (w * h);

            if (visible > fraction) {
                video.play();
            } else {
                video.pause();
            }

        }

    }
    window.addEventListener('scroll', checkScroll, false);
    window.addEventListener('resize', checkScroll, false);
</script>
<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
        type="text/javascript"></script>
</body>
</html>
